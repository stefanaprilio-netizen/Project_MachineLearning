{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705e7d42",
   "metadata": {},
   "source": [
    "# MLOps Plan: KROM Bank Indonesia Stock - Notebook\n",
    "# Notebook ini berisi rencana MLOps, EDA cepat, baseline modeling, dan langkah produksi.\n",
    "# Dataset: files ditemukan di folder `dataset/` (mis: `BBSI.JK.csv`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Persiapan lingkungan & install library\n",
    "\n",
    "# Perbarui pip dan install dependencies yang dibutuhkan. Jalankan cell ini di venv proyek (lihat `configure_python_environment`).\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def run(cmd):\n",
    "    print('>',' '.join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "# Update pip\n",
    "try:\n",
    "    run([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip'])\n",
    "except Exception as e:\n",
    "    print('Gagal update pip:', e)\n",
    "\n",
    "# Install packages (guarded installs)\n",
    "packages = [\n",
    "    'pandas', 'numpy', 'matplotlib', 'seaborn', 'scikit-learn', 'statsmodels',\n",
    "    'prophet>=1.1', 'tensorflow', 'mlflow', 'optuna', 'joblib', 'pandera', 'lightgbm'\n",
    "]\n",
    "try:\n",
    "    run([sys.executable, '-m', 'pip', 'install'] + packages)\n",
    "except Exception as e:\n",
    "    print('Instalasi paket gagal (lanjutkan jika sudah terpasang):', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82cc9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports umum untuk notebook\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('.')\n",
    "DATA_DIR = ROOT / 'dataset'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "ARTIFACTS_DIR = ROOT / 'artifacts'\n",
    "for d in [DATA_DIR, MODELS_DIR, ARTIFACTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('DATA_DIR:', DATA_DIR.resolve())\n",
    "print('Found files:', list(DATA_DIR.glob('*'))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2074220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: cari CSV utama (prioritaskan daily CSV)\n",
    "from glob import glob\n",
    "csv_candidates = sorted(glob(str(DATA_DIR / '*.csv')))\n",
    "print('CSV candidates:', csv_candidates)\n",
    "\n",
    "# Preferensi: file yang mengandung '.JK.csv' atau the largest daily file\n",
    "def get_csv_path():\n",
    "    for p in csv_candidates:\n",
    "        if p.endswith('.csv') and '.JK' in os.path.basename(p) and '_monthly' not in p and '_weekly' not in p:\n",
    "            return Path(p)\n",
    "    # fallback: first csv\n",
    "    return Path(csv_candidates[0]) if csv_candidates else None\n",
    "\n",
    "CSV_PATH = get_csv_path()\n",
    "print('Using CSV:', CSV_PATH)\n",
    "\n",
    "# Muat CSV jika ada\n",
    "if CSV_PATH and CSV_PATH.exists():\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print('Loaded rows,cols:', df.shape)\n",
    "    display(df.head())\n",
    "    display(df.info())\n",
    "else:\n",
    "    print('CSV not found in dataset/, pastikan sudah diunduh.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca105935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan salinan raw\n",
    "RAW_PATH = DATA_DIR / 'raw.csv'\n",
    "if 'df' in globals():\n",
    "    df.to_csv(RAW_PATH, index=False)\n",
    "    print('Saved raw snapshot to', RAW_PATH)\n",
    "\n",
    "# Basic EDA: missing, describe, date range\n",
    "if 'df' in globals():\n",
    "    print('\\nMissing per column:\\n', df.isna().sum())\n",
    "    display(df.describe())\n",
    "    # Cari kolom tanggal yang paling umum (case-insensitive)\n",
    "    date_cols = [c for c in df.columns if 'date' in c.lower() or 'tanggal' in c.lower()]\n",
    "    print('Date columns candidates:', date_cols)\n",
    "    if date_cols:\n",
    "        date_col = date_cols[0]\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        if df[date_col].isna().all():\n",
    "            warnings.warn(f\"Kolom '{date_col}' tidak dapat diparse ke datetime (semua NaT). Periksa kolom lain atau format tanggal.\")\n",
    "        else:\n",
    "            print('Date range:', df[date_col].min(), 'to', df[date_col].max())\n",
    "    else:\n",
    "        warnings.warn('Tidak menemukan kolom tanggal otomatis; pastikan file memiliki kolom tanggal.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38993cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: set date index, sort, resample daily, fill missing\n",
    "# This cell creates `df_daily` used by feature engineering.\n",
    "if 'df' in globals():\n",
    "    # detect date column again (from EDA)\n",
    "    possible_dates = [c for c in df.columns if 'date' in c.lower() or 'tanggal' in c.lower()]\n",
    "    date_col = possible_dates[0] if possible_dates else df.columns[0]\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    if df[date_col].isna().sum() > 0:\n",
    "        warnings.warn('Beberapa tanggal tidak ter-parse; baris akan di-drop sebelum resample.')\n",
    "        df = df.loc[df[date_col].notna()].copy()\n",
    "    df = df.sort_values(date_col).set_index(date_col)\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    if not numeric_cols:\n",
    "        warnings.warn('Tidak ada kolom numerik; feature engineering akan dibatalkan.')\n",
    "    else:\n",
    "        # resample ke harian dan forward-fill untuk missing timestamps\n",
    "        df_daily = df[numeric_cols].resample('D').ffill()\n",
    "        print('df_daily shape:', df_daily.shape)\n",
    "else:\n",
    "    warnings.warn('`df` tidak ditemukan; pastikan CSV sudah dimuat.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a671d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: returns, lags, rolling\n",
    "if 'df_daily' in globals():\n",
    "    df_feat = df_daily.copy()\n",
    "    # ensure 'Close' exists\n",
    "    close_col = None\n",
    "    for c in ['Close','close','Adj Close','adj_close','Close*']:\n",
    "        if c in df_feat.columns:\n",
    "            close_col = c\n",
    "            break\n",
    "    if close_col is None:\n",
    "        # fallback to first numeric column\n",
    "        close_col = df_feat.select_dtypes('number').columns[0]\n",
    "        warnings.warn(f\"Tidak menemukan kolom 'Close' eksplisit. Menggunakan '{close_col}' sebagai target.\")\n",
    "    # require enough data for lags and rolling calculations\n",
    "    min_required = 21\n",
    "    if len(df_feat) < min_required:\n",
    "        raise ValueError(f\"Data terlalu sedikit untuk feature engineering (butuh >= {min_required}, punya {len(df_feat)})\")\n",
    "    df_feat['return'] = df_feat[close_col].pct_change()\n",
    "    for lag in [1,2,3,5,7]:\n",
    "        df_feat[f'lag_{lag}'] = df_feat[close_col].shift(lag)\n",
    "    df_feat['sma_7'] = df_feat[close_col].rolling(7).mean()\n",
    "    df_feat['sma_21'] = df_feat[close_col].rolling(21).mean()\n",
    "    df_feat = df_feat.dropna().copy()\n",
    "    display(df_feat[[close_col,'return','sma_7','sma_21']].head())\n",
    "else:\n",
    "    warnings.warn('`df_daily` tidak tersedia; jalankan sel preprocessing terlebih dahulu.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e982847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val/test split (time-series aware) and baseline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# safe metric functions\n",
    "def rmse(a,b):\n",
    "    return sqrt(mean_squared_error(a,b))\n",
    "\n",
    "def mape(a,b):\n",
    "    a = np.array(a, dtype=float)\n",
    "    b = np.array(b, dtype=float)\n",
    "    # avoid division by zero: ignore positions where a == 0\n",
    "    mask = a != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((a[mask]-b[mask])/a[mask]))*100\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'rmse': rmse(y_true, y_pred),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'mape': mape(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "if 'df_feat' in globals():\n",
    "    y = df_feat[close_col]\n",
    "    X = df_feat.drop(columns=[close_col])\n",
    "    n = len(df_feat)\n",
    "    train_end = int(n*0.7)\n",
    "    val_end = train_end + int(n*0.15)\n",
    "    X_train, y_train = X.iloc[:train_end], y.iloc[:train_end]\n",
    "    X_val, y_val = X.iloc[train_end:val_end], y.iloc[train_end:val_end]\n",
    "    X_test, y_test = X.iloc[val_end:], y.iloc[val_end:]\n",
    "    print('Train/Val/Test sizes:', len(X_train), len(X_val), len(X_test))\n",
    "\n",
    "# Baseline: naive forecast (predict t+1 = t)\n",
    "if 'y_test' in globals() and len(y_test)>0:\n",
    "    naive_pred = y_test.shift(1).fillna(method='bfill')\n",
    "    print('Naive metrics:', compute_metrics(y_test.values, naive_pred.values))\n",
    "else:\n",
    "    print('Tidak ada data test untuk mengevaluasi baseline.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff948c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest baseline + save model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "rf_rmse = None\n",
    "\n",
    "if 'X_train' in globals() and len(X_test)>0:\n",
    "    pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    metrics = compute_metrics(y_test.values, preds)\n",
    "    rf_rmse = metrics['rmse']\n",
    "    print('RF metrics:', metrics)\n",
    "    model_path = MODELS_DIR / f'rf_baseline.pkl'\n",
    "    joblib.dump(pipe, model_path)\n",
    "    print('Saved model to', model_path)\n",
    "else:\n",
    "    warnings.warn('Tidak cukup data untuk melatih RandomForest baseline.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d7f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet example (fallback if not installed) and quick LSTM skeleton\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    prophet_available = True\n",
    "except Exception as e:\n",
    "    prophet_available = False\n",
    "    print('Prophet not available:', e)\n",
    "\n",
    "if prophet_available and 'df' in globals():\n",
    "    # robust creation of df for Prophet: ensure a ds and y\n",
    "    pdf = df.reset_index().rename(columns={df.index.name if df.index.name else df.columns[0]: 'ds'})\n",
    "    if close_col not in pdf.columns:\n",
    "        warnings.warn('Kolom target tidak ditemukan pada dataframe untuk Prophet.')\n",
    "    else:\n",
    "        pdf = pdf[['ds', close_col]].rename(columns={close_col: 'y'})\n",
    "        pdf = pdf.dropna()\n",
    "        train_size = int(len(pdf)*0.7)\n",
    "        p_train = pdf.iloc[:train_size]\n",
    "        p_test = pdf.iloc[train_size:]\n",
    "        try:\n",
    "            m = Prophet()\n",
    "            m.fit(p_train)\n",
    "            future = m.make_future_dataframe(periods=len(p_test), freq='D')\n",
    "            fcst = m.predict(future)\n",
    "            fc_pred = fcst.set_index('ds')['yhat'].iloc[-len(p_test):]\n",
    "            print('Prophet RMSE:', rmse(p_test['y'].values, fc_pred.values))\n",
    "        except Exception as e:\n",
    "            warnings.warn(f'Prophet run failed: {e}')\n",
    "\n",
    "# LSTM skeleton (tensorflow)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    tf_available = True\n",
    "except Exception as e:\n",
    "    tf_available = False\n",
    "    print('TensorFlow not available:', e)\n",
    "\n",
    "if tf_available and 'df_feat' in globals() and len(df_feat) > 50:\n",
    "    # prepare simple windowed dataset\n",
    "    values = df_feat[close_col].values\n",
    "    window = 21\n",
    "    Xw, yw = [], []\n",
    "    for i in range(window, len(values)):\n",
    "        Xw.append(values[i-window:i])\n",
    "        yw.append(values[i])\n",
    "    Xw = np.array(Xw)[..., np.newaxis]\n",
    "    yw = np.array(yw)\n",
    "    # split\n",
    "    split = int(0.7*len(Xw))\n",
    "    Xtr, ytr = Xw[:split], yw[:split]\n",
    "    Xte, yte = Xw[split:], yw[split:]\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(window,1)),\n",
    "        tf.keras.layers.LSTM(32),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    # NOTE: training LSTM can be slow; ensure you have resources\n",
    "    # train briefly\n",
    "    history = model.fit(Xtr, ytr, epochs=5, validation_split=0.1)\n",
    "    # save model\n",
    "    model.save(MODELS_DIR / 'lstm_baseline')\n",
    "    print('LSTM trained and saved')\n",
    "else:\n",
    "    if tf_available:\n",
    "        warnings.warn('Data tidak cukup besar untuk LSTM (butuh >50 baris fitur).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow quick setup & logging example (local)\n",
    "try:\n",
    "    import mlflow\n",
    "    mlflow.set_tracking_uri('file://' + str(ARTIFACTS_DIR / 'mlruns'))\n",
    "    print('MLflow tracking URI:', mlflow.get_tracking_uri())\n",
    "    with mlflow.start_run(run_name='rf_baseline'):\n",
    "        mlflow.log_param('model', 'RandomForest')\n",
    "        if 'rf_rmse' in globals() and rf_rmse is not None:\n",
    "            mlflow.log_metric('rf_rmse', float(rf_rmse))\n",
    "        else:\n",
    "            mlflow.log_metric('rf_rmse', -1)\n",
    "        if 'model_path' in globals() and Path(model_path).exists():\n",
    "            mlflow.log_artifact(str(model_path))\n",
    "        else:\n",
    "            warnings.warn('model_path tidak ada; tidak ada artefak model yang dilog.')\n",
    "except Exception as e:\n",
    "    print('MLflow not available or failed:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32157a9a",
   "metadata": {},
   "source": [
    "## Next steps & checklist\n",
    "\n",
    "- Validasi schema dengan `pandera`.\n",
    "- Tambahkan unit tests (`pytest`) untuk fungsi preprocessing & feature engineering.\n",
    "- Tambahkan CI (GitHub Actions) untuk lint, tests, build docker image.\n",
    "- Tambahkan Dockerfile dan FastAPI endpoint untuk `predict`.\n",
    "- Setup monitoring: drift detection dan retrain triggers.\n",
    "\n",
    "---\n",
    "\n",
    "**Catatan:** jalankan notebook cell per cell di environment yang benar; beberapa paket (Prophet, TensorFlow) mungkin membutuhkan dependencies OS tambahan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f807058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model skeleton (Conv1D) + save preprocessing artifacts\n",
    "# This cell does NOT run automatically; run manually after EDA & preprocessing cells.\n",
    "import joblib\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "    tf_available = True\n",
    "except Exception as e:\n",
    "    tf_available = False\n",
    "    print('TensorFlow not available:', e)\n",
    "\n",
    "if 'df_feat' in globals():\n",
    "    # robustly determine target and features\n",
    "    if 'close_col' not in globals():\n",
    "        # try to infer\n",
    "        possible = [c for c in df_feat.columns if 'close' in c.lower() or 'adj' in c.lower()]\n",
    "        close_col_local = possible[0] if possible else df_feat.select_dtypes('number').columns[0]\n",
    "    else:\n",
    "        close_col_local = close_col\n",
    "    feature_cols = [c for c in df_feat.columns if c != close_col_local]\n",
    "\n",
    "    # Save preprocessed dataframe\n",
    "    ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    preproc_path = ARTIFACTS_DIR / 'preprocessed.parquet'\n",
    "    df_feat.to_parquet(preproc_path)\n",
    "    print('Saved preprocessed data to', preproc_path)\n",
    "\n",
    "    # Save feature list\n",
    "    (ARTIFACTS_DIR / 'feature_list.txt').write_text('\\n'.join(feature_cols))\n",
    "    print('Saved feature list to', ARTIFACTS_DIR / 'feature_list.txt')\n",
    "\n",
    "    # Prepare data for CNN: use sliding window on features to predict next close\n",
    "    window = 21\n",
    "    values_X = df_feat[feature_cols].values\n",
    "    values_y = df_feat[close_col_local].values\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window, len(values_y)):\n",
    "        Xs.append(values_X[i-window:i])\n",
    "        ys.append(values_y[i])\n",
    "    Xs = np.array(Xs)\n",
    "    ys = np.array(ys)\n",
    "    print('Prepared CNN dataset shapes', Xs.shape, ys.shape)\n",
    "\n",
    "    # Fit a StandardScaler on features and save\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # reshape for scaler: combine windows\n",
    "    n_samples, n_steps, n_features = Xs.shape\n",
    "    Xs_2d = Xs.reshape(-1, n_features)\n",
    "    scaler.fit(Xs_2d)\n",
    "    joblib.dump(scaler, ARTIFACTS_DIR / 'scaler.joblib')\n",
    "    print('Saved scaler to', ARTIFACTS_DIR / 'scaler.joblib')\n",
    "\n",
    "    # Optionally train a small Conv1D model if TF available\n",
    "    if tf_available and len(Xs) > 0:\n",
    "        # scale X\n",
    "        Xs_scaled = scaler.transform(Xs_2d).reshape(n_samples, n_steps, n_features)\n",
    "        split = int(0.7 * n_samples)\n",
    "        Xtr, Xte = Xs_scaled[:split], Xs_scaled[split:]\n",
    "        ytr, yte = ys[:split], ys[split:]\n",
    "\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=(n_steps, n_features)),\n",
    "            layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
    "            layers.MaxPooling1D(pool_size=2),\n",
    "            layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "            layers.GlobalAveragePooling1D(),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        # NOTE: keep epochs small for smoke test\n",
    "        model.fit(Xtr, ytr, epochs=5, validation_data=(Xte, yte))\n",
    "        cnn_path = MODELS_DIR / 'cnn_conv1d'\n",
    "        model.save(cnn_path)\n",
    "        print('Saved CNN model to', cnn_path)\n",
    "    else:\n",
    "        print('Skipping CNN training (TensorFlow not available).')\n",
    "else:\n",
    "    print('df_feat not found; run preprocessing/feature engineering cells first.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

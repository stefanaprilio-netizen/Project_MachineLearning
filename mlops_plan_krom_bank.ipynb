{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705e7d42",
   "metadata": {},
   "source": [
    "# MLOps Plan: KROM Bank Indonesia Stock - Notebook\n",
    "# Notebook ini berisi rencana MLOps, EDA cepat, baseline modeling, dan langkah produksi.\n",
    "# Dataset: files ditemukan di folder `dataset/` (mis: `BBSI.JK.csv`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3daa4e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> c:\\Users\\ASUS\\Documents\\MyProject_Machine\\Project_MachineLearning-1\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n",
      "> c:\\Users\\ASUS\\Documents\\MyProject_Machine\\Project_MachineLearning-1\\.venv\\Scripts\\python.exe -m pip install pandas numpy matplotlib seaborn scikit-learn statsmodels prophet>=1.1 tensorflow mlflow optuna joblib pandera lightgbm\n"
     ]
    }
   ],
   "source": [
    "## Persiapan lingkungan & install library\n",
    "\n",
    "# Perbarui pip dan install dependencies yang dibutuhkan. Jalankan cell ini di venv proyek (lihat `configure_python_environment`).\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def run(cmd):\n",
    "    print('>',' '.join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "# Update pip\n",
    "try:\n",
    "    run([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip'])\n",
    "except Exception as e:\n",
    "    print('Gagal update pip:', e)\n",
    "\n",
    "# Install packages (guarded installs)\n",
    "packages = [\n",
    "    'pandas', 'numpy', 'matplotlib', 'seaborn', 'scikit-learn', 'statsmodels',\n",
    "    'prophet>=1.1', 'tensorflow', 'mlflow', 'optuna', 'joblib', 'pandera', 'lightgbm'\n",
    "]\n",
    "try:\n",
    "    run([sys.executable, '-m', 'pip', 'install'] + packages)\n",
    "except Exception as e:\n",
    "    print('Instalasi paket gagal (lanjutkan jika sudah terpasang):', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e82cc9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: C:\\Users\\ASUS\\Documents\\MyProject_Machine\\Project_MachineLearning-1\\dataset\n",
      "Found files: [WindowsPath('dataset/BBSI.JK.csv'), WindowsPath('dataset/BBSI.JK.parquet'), WindowsPath('dataset/BBSI.JK_monthly.csv'), WindowsPath('dataset/BBSI.JK_monthly.parquet'), WindowsPath('dataset/BBSI.JK_weekly.csv'), WindowsPath('dataset/BBSI.JK_weekly.parquet'), WindowsPath('dataset/raw.csv'), WindowsPath('dataset/README.txt'), WindowsPath('dataset/run-metadata.json'), WindowsPath('dataset/run-metadata_monthly.json')]\n"
     ]
    }
   ],
   "source": [
    "# Imports umum untuk notebook\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('.')\n",
    "DATA_DIR = ROOT / 'dataset'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "ARTIFACTS_DIR = ROOT / 'artifacts'\n",
    "for d in [DATA_DIR, MODELS_DIR, ARTIFACTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('DATA_DIR:', DATA_DIR.resolve())\n",
    "print('Found files:', list(DATA_DIR.glob('*'))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2074220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV candidates: ['dataset\\\\BBSI.JK.csv', 'dataset\\\\BBSI.JK_monthly.csv', 'dataset\\\\BBSI.JK_weekly.csv', 'dataset\\\\raw.csv']\n",
      "Using CSV: dataset\\BBSI.JK.csv\n",
      "Loaded rows,cols: (1304, 8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "open",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "high",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "low",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "adjclose",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "volume",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ingested_at_utc",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "17b3a119-f46d-480d-8852-a6023f2b2579",
       "rows": [
        [
         "0",
         "2020-09-07",
         "549.8370361328125",
         "549.8370361328125",
         "458.1975402832031",
         "549.8370361328125",
         "549.8370361328125",
         "29516635",
         "2026-02-04 06:58:01.058673+00:00"
        ],
        [
         "1",
         "2020-09-08",
         "595.6567993164062",
         "687.2963256835938",
         "595.6567993164062",
         "687.2963256835938",
         "687.2963256835938",
         "19232861",
         "2026-02-04 06:58:01.058673+00:00"
        ],
        [
         "2",
         "2020-09-09",
         "733.1160888671875",
         "856.8294067382812",
         "687.2963256835938",
         "765.1898803710938",
         "765.1898803710938",
         "33271020",
         "2026-02-04 06:58:01.058673+00:00"
        ],
        [
         "3",
         "2020-09-10",
         "765.1898803710938",
         "797.2637329101562",
         "714.7881469726562",
         "714.7881469726562",
         "714.7881469726562",
         "1050747",
         "2026-02-04 06:58:01.058673+00:00"
        ],
        [
         "4",
         "2020-09-11",
         "668.9683837890625",
         "668.9683837890625",
         "668.9683837890625",
         "668.9683837890625",
         "668.9683837890625",
         "500439",
         "2026-02-04 06:58:01.058673+00:00"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ingested_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>549.837036</td>\n",
       "      <td>549.837036</td>\n",
       "      <td>458.197540</td>\n",
       "      <td>549.837036</td>\n",
       "      <td>549.837036</td>\n",
       "      <td>29516635</td>\n",
       "      <td>2026-02-04 06:58:01.058673+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-08</td>\n",
       "      <td>595.656799</td>\n",
       "      <td>687.296326</td>\n",
       "      <td>595.656799</td>\n",
       "      <td>687.296326</td>\n",
       "      <td>687.296326</td>\n",
       "      <td>19232861</td>\n",
       "      <td>2026-02-04 06:58:01.058673+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>733.116089</td>\n",
       "      <td>856.829407</td>\n",
       "      <td>687.296326</td>\n",
       "      <td>765.189880</td>\n",
       "      <td>765.189880</td>\n",
       "      <td>33271020</td>\n",
       "      <td>2026-02-04 06:58:01.058673+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>765.189880</td>\n",
       "      <td>797.263733</td>\n",
       "      <td>714.788147</td>\n",
       "      <td>714.788147</td>\n",
       "      <td>714.788147</td>\n",
       "      <td>1050747</td>\n",
       "      <td>2026-02-04 06:58:01.058673+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>668.968384</td>\n",
       "      <td>668.968384</td>\n",
       "      <td>668.968384</td>\n",
       "      <td>668.968384</td>\n",
       "      <td>668.968384</td>\n",
       "      <td>500439</td>\n",
       "      <td>2026-02-04 06:58:01.058673+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        open        high         low       close    adjclose  \\\n",
       "0  2020-09-07  549.837036  549.837036  458.197540  549.837036  549.837036   \n",
       "1  2020-09-08  595.656799  687.296326  595.656799  687.296326  687.296326   \n",
       "2  2020-09-09  733.116089  856.829407  687.296326  765.189880  765.189880   \n",
       "3  2020-09-10  765.189880  797.263733  714.788147  714.788147  714.788147   \n",
       "4  2020-09-11  668.968384  668.968384  668.968384  668.968384  668.968384   \n",
       "\n",
       "     volume                   ingested_at_utc  \n",
       "0  29516635  2026-02-04 06:58:01.058673+00:00  \n",
       "1  19232861  2026-02-04 06:58:01.058673+00:00  \n",
       "2  33271020  2026-02-04 06:58:01.058673+00:00  \n",
       "3   1050747  2026-02-04 06:58:01.058673+00:00  \n",
       "4    500439  2026-02-04 06:58:01.058673+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1304 entries, 0 to 1303\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Date             1304 non-null   object \n",
      " 1   open             1304 non-null   float64\n",
      " 2   high             1304 non-null   float64\n",
      " 3   low              1304 non-null   float64\n",
      " 4   close            1304 non-null   float64\n",
      " 5   adjclose         1304 non-null   float64\n",
      " 6   volume           1304 non-null   int64  \n",
      " 7   ingested_at_utc  1304 non-null   object \n",
      "dtypes: float64(5), int64(1), object(2)\n",
      "memory usage: 81.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper: cari CSV utama (prioritaskan daily CSV)\n",
    "from glob import glob\n",
    "csv_candidates = sorted(glob(str(DATA_DIR / '*.csv')))\n",
    "print('CSV candidates:', csv_candidates)\n",
    "\n",
    "# Preferensi: file yang mengandung '.JK.csv' atau the largest daily file\n",
    "def get_csv_path():\n",
    "    for p in csv_candidates:\n",
    "        if p.endswith('.csv') and '.JK' in os.path.basename(p) and '_monthly' not in p and '_weekly' not in p:\n",
    "            return Path(p)\n",
    "    # fallback: first csv\n",
    "    return Path(csv_candidates[0]) if csv_candidates else None\n",
    "\n",
    "CSV_PATH = get_csv_path()\n",
    "print('Using CSV:', CSV_PATH)\n",
    "\n",
    "# Muat CSV jika ada\n",
    "if CSV_PATH and CSV_PATH.exists():\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print('Loaded rows,cols:', df.shape)\n",
    "    display(df.head())\n",
    "    display(df.info())\n",
    "else:\n",
    "    print('CSV not found in dataset/, pastikan sudah diunduh.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca105935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw snapshot to dataset\\raw.csv\n",
      "\n",
      "Missing per column:\n",
      " Date               0\n",
      "open               0\n",
      "high               0\n",
      "low                0\n",
      "close              0\n",
      "adjclose           0\n",
      "volume             0\n",
      "ingested_at_utc    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "open",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "high",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "low",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "adjclose",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "volume",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "13361d3b-ddf3-4ced-9860-4accd357f20d",
       "rows": [
        [
         "count",
         "1304.0",
         "1304.0",
         "1304.0",
         "1304.0",
         "1304.0",
         "1304.0"
        ],
        [
         "mean",
         "3690.913606163914",
         "3763.538158440151",
         "3612.5498221227726",
         "3701.9245590724827",
         "3701.9245590724827",
         "130725.37116564417"
        ],
        [
         "std",
         "1172.4491681851189",
         "1187.2318658345807",
         "1149.1500829937186",
         "1170.7896711322817",
         "1170.7896711322817",
         "1376887.3524102317"
        ],
        [
         "min",
         "508.5992736816406",
         "522.34521484375",
         "458.1975402832031",
         "508.5992736816406",
         "508.5992736816406",
         "0.0"
        ],
        [
         "25%",
         "3379.2130737304688",
         "3441.04150390625",
         "3300.0",
         "3380.0",
         "3380.0",
         "3400.0"
        ],
        [
         "50%",
         "4000.0",
         "4020.0",
         "3900.752197265625",
         "4000.0",
         "4000.0",
         "13500.0"
        ],
        [
         "75%",
         "4260.997802734375",
         "4312.8065185546875",
         "4200.0",
         "4280.0",
         "4280.0",
         "37836.75"
        ],
        [
         "max",
         "8070.1025390625",
         "8070.1025390625",
         "6817.84521484375",
         "7328.0244140625",
         "7328.0244140625",
         "33271020.0"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1.304000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3690.913606</td>\n",
       "      <td>3763.538158</td>\n",
       "      <td>3612.549822</td>\n",
       "      <td>3701.924559</td>\n",
       "      <td>3701.924559</td>\n",
       "      <td>1.307254e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1172.449168</td>\n",
       "      <td>1187.231866</td>\n",
       "      <td>1149.150083</td>\n",
       "      <td>1170.789671</td>\n",
       "      <td>1170.789671</td>\n",
       "      <td>1.376887e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>508.599274</td>\n",
       "      <td>522.345215</td>\n",
       "      <td>458.197540</td>\n",
       "      <td>508.599274</td>\n",
       "      <td>508.599274</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3379.213074</td>\n",
       "      <td>3441.041504</td>\n",
       "      <td>3300.000000</td>\n",
       "      <td>3380.000000</td>\n",
       "      <td>3380.000000</td>\n",
       "      <td>3.400000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4020.000000</td>\n",
       "      <td>3900.752197</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>1.350000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4260.997803</td>\n",
       "      <td>4312.806519</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4280.000000</td>\n",
       "      <td>4280.000000</td>\n",
       "      <td>3.783675e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8070.102539</td>\n",
       "      <td>8070.102539</td>\n",
       "      <td>6817.845215</td>\n",
       "      <td>7328.024414</td>\n",
       "      <td>7328.024414</td>\n",
       "      <td>3.327102e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              open         high          low        close     adjclose  \\\n",
       "count  1304.000000  1304.000000  1304.000000  1304.000000  1304.000000   \n",
       "mean   3690.913606  3763.538158  3612.549822  3701.924559  3701.924559   \n",
       "std    1172.449168  1187.231866  1149.150083  1170.789671  1170.789671   \n",
       "min     508.599274   522.345215   458.197540   508.599274   508.599274   \n",
       "25%    3379.213074  3441.041504  3300.000000  3380.000000  3380.000000   \n",
       "50%    4000.000000  4020.000000  3900.752197  4000.000000  4000.000000   \n",
       "75%    4260.997803  4312.806519  4200.000000  4280.000000  4280.000000   \n",
       "max    8070.102539  8070.102539  6817.845215  7328.024414  7328.024414   \n",
       "\n",
       "             volume  \n",
       "count  1.304000e+03  \n",
       "mean   1.307254e+05  \n",
       "std    1.376887e+06  \n",
       "min    0.000000e+00  \n",
       "25%    3.400000e+03  \n",
       "50%    1.350000e+04  \n",
       "75%    3.783675e+04  \n",
       "max    3.327102e+07  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date columns candidates: ['Date']\n",
      "Date range: 2020-09-07 00:00:00 to 2026-02-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Simpan salinan raw\n",
    "RAW_PATH = DATA_DIR / 'raw.csv'\n",
    "if 'df' in globals():\n",
    "    df.to_csv(RAW_PATH, index=False)\n",
    "    print('Saved raw snapshot to', RAW_PATH)\n",
    "\n",
    "# Basic EDA: missing, describe, date range\n",
    "if 'df' in globals():\n",
    "    print('\\nMissing per column:\\n', df.isna().sum())\n",
    "    display(df.describe())\n",
    "    # Cari kolom tanggal yang paling umum (case-insensitive)\n",
    "    date_cols = [c for c in df.columns if 'date' in c.lower() or 'tanggal' in c.lower()]\n",
    "    print('Date columns candidates:', date_cols)\n",
    "    if date_cols:\n",
    "        date_col = date_cols[0]\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        if df[date_col].isna().all():\n",
    "            warnings.warn(f\"Kolom '{date_col}' tidak dapat diparse ke datetime (semua NaT). Periksa kolom lain atau format tanggal.\")\n",
    "        else:\n",
    "            print('Date range:', df[date_col].min(), 'to', df[date_col].max())\n",
    "    else:\n",
    "        warnings.warn('Tidak menemukan kolom tanggal otomatis; pastikan file memiliki kolom tanggal.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38993cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_daily shape: (1976, 6)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: set date index, sort, resample daily, fill missing\n",
    "# This cell creates `df_daily` used by feature engineering.\n",
    "if 'df' in globals():\n",
    "    # detect date column again (from EDA)\n",
    "    possible_dates = [c for c in df.columns if 'date' in c.lower() or 'tanggal' in c.lower()]\n",
    "    date_col = possible_dates[0] if possible_dates else df.columns[0]\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    if df[date_col].isna().sum() > 0:\n",
    "        warnings.warn('Beberapa tanggal tidak ter-parse; baris akan di-drop sebelum resample.')\n",
    "        df = df.loc[df[date_col].notna()].copy()\n",
    "    df = df.sort_values(date_col).set_index(date_col)\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    if not numeric_cols:\n",
    "        warnings.warn('Tidak ada kolom numerik; feature engineering akan dibatalkan.')\n",
    "    else:\n",
    "        # resample ke harian dan forward-fill untuk missing timestamps\n",
    "        df_daily = df[numeric_cols].resample('D').ffill()\n",
    "        print('df_daily shape:', df_daily.shape)\n",
    "else:\n",
    "    warnings.warn('`df` tidak ditemukan; pastikan CSV sudah dimuat.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a671d977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "return",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sma_7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sma_21",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "1c1aeaac-96ad-4602-8bb1-192d9b7984cd",
       "rows": [
        [
         "2020-09-27 00:00:00",
         "568.1649780273438",
         "0.0",
         "549.1824907575335",
         "599.8023986816406"
        ],
        [
         "2020-09-28 00:00:00",
         "586.4928588867188",
         "0.032258026397559814",
         "560.3101457868304",
         "601.5479140508743"
        ],
        [
         "2020-09-29 00:00:00",
         "604.8207397460938",
         "0.031249964226614857",
         "572.0923636300223",
         "597.6205051967075"
        ],
        [
         "2020-09-30 00:00:00",
         "636.8945922851562",
         "0.0530303450779932",
         "584.5291573660714",
         "591.5112057640439"
        ],
        [
         "2020-10-01 00:00:00",
         "623.148681640625",
         "-0.021582708993039823",
         "593.693115234375",
         "587.1474217006138"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>return</th>\n",
       "      <th>sma_7</th>\n",
       "      <th>sma_21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-09-27</th>\n",
       "      <td>568.164978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>549.182491</td>\n",
       "      <td>599.802399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-28</th>\n",
       "      <td>586.492859</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>560.310146</td>\n",
       "      <td>601.547914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>604.820740</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>572.092364</td>\n",
       "      <td>597.620505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>636.894592</td>\n",
       "      <td>0.053030</td>\n",
       "      <td>584.529157</td>\n",
       "      <td>591.511206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01</th>\n",
       "      <td>623.148682</td>\n",
       "      <td>-0.021583</td>\n",
       "      <td>593.693115</td>\n",
       "      <td>587.147422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 close    return       sma_7      sma_21\n",
       "Date                                                    \n",
       "2020-09-27  568.164978  0.000000  549.182491  599.802399\n",
       "2020-09-28  586.492859  0.032258  560.310146  601.547914\n",
       "2020-09-29  604.820740  0.031250  572.092364  597.620505\n",
       "2020-09-30  636.894592  0.053030  584.529157  591.511206\n",
       "2020-10-01  623.148682 -0.021583  593.693115  587.147422"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature engineering: returns, lags, rolling\n",
    "if 'df_daily' in globals():\n",
    "    df_feat = df_daily.copy()\n",
    "    # ensure 'Close' exists\n",
    "    close_col = None\n",
    "    for c in ['Close','close','Adj Close','adj_close','Close*']:\n",
    "        if c in df_feat.columns:\n",
    "            close_col = c\n",
    "            break\n",
    "    if close_col is None:\n",
    "        # fallback to first numeric column\n",
    "        close_col = df_feat.select_dtypes('number').columns[0]\n",
    "        warnings.warn(f\"Tidak menemukan kolom 'Close' eksplisit. Menggunakan '{close_col}' sebagai target.\")\n",
    "    # require enough data for lags and rolling calculations\n",
    "    min_required = 21\n",
    "    if len(df_feat) < min_required:\n",
    "        raise ValueError(f\"Data terlalu sedikit untuk feature engineering (butuh >= {min_required}, punya {len(df_feat)})\")\n",
    "    df_feat['return'] = df_feat[close_col].pct_change()\n",
    "    for lag in [1,2,3,5,7]:\n",
    "        df_feat[f'lag_{lag}'] = df_feat[close_col].shift(lag)\n",
    "    df_feat['sma_7'] = df_feat[close_col].rolling(7).mean()\n",
    "    df_feat['sma_21'] = df_feat[close_col].rolling(21).mean()\n",
    "    df_feat = df_feat.dropna().copy()\n",
    "    display(df_feat[[close_col,'return','sma_7','sma_21']].head())\n",
    "else:\n",
    "    warnings.warn('`df_daily` tidak tersedia; jalankan sel preprocessing terlebih dahulu.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e982847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test sizes: 1369 293 294\n",
      "Naive metrics: {'rmse': 97.58826460059171, 'mae': 36.42857142857143, 'mape': 0.8639266977681075}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_4184\\3697970242.py:37: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  naive_pred = y_test.shift(1).fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "# Train/val/test split (time-series aware) and baseline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# safe metric functions\n",
    "def rmse(a,b):\n",
    "    return sqrt(mean_squared_error(a,b))\n",
    "\n",
    "def mape(a,b):\n",
    "    a = np.array(a, dtype=float)\n",
    "    b = np.array(b, dtype=float)\n",
    "    # avoid division by zero: ignore positions where a == 0\n",
    "    mask = a != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((a[mask]-b[mask])/a[mask]))*100\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'rmse': rmse(y_true, y_pred),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'mape': mape(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "if 'df_feat' in globals():\n",
    "    y = df_feat[close_col]\n",
    "    X = df_feat.drop(columns=[close_col])\n",
    "    n = len(df_feat)\n",
    "    train_end = int(n*0.7)\n",
    "    val_end = train_end + int(n*0.15)\n",
    "    X_train, y_train = X.iloc[:train_end], y.iloc[:train_end]\n",
    "    X_val, y_val = X.iloc[train_end:val_end], y.iloc[train_end:val_end]\n",
    "    X_test, y_test = X.iloc[val_end:], y.iloc[val_end:]\n",
    "    print('Train/Val/Test sizes:', len(X_train), len(X_val), len(X_test))\n",
    "\n",
    "# Baseline: naive forecast (predict t+1 = t)\n",
    "if 'y_test' in globals() and len(y_test)>0:\n",
    "    naive_pred = y_test.shift(1).fillna(method='bfill')\n",
    "    print('Naive metrics:', compute_metrics(y_test.values, naive_pred.values))\n",
    "else:\n",
    "    print('Tidak ada data test untuk mengevaluasi baseline.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ff948c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF metrics: {'rmse': 11.228325775936217, 'mae': 8.874453623246172, 'mape': 0.21877368484402945}\n",
      "Saved model to models\\rf_baseline.pkl\n"
     ]
    }
   ],
   "source": [
    "# RandomForest baseline + save model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "rf_rmse = None\n",
    "\n",
    "if 'X_train' in globals() and len(X_test)>0:\n",
    "    pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    metrics = compute_metrics(y_test.values, preds)\n",
    "    rf_rmse = metrics['rmse']\n",
    "    print('RF metrics:', metrics)\n",
    "    model_path = MODELS_DIR / f'rf_baseline.pkl'\n",
    "    joblib.dump(pipe, model_path)\n",
    "    print('Saved model to', model_path)\n",
    "else:\n",
    "    warnings.warn('Tidak cukup data untuk melatih RandomForest baseline.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b5d7f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Documents\\MyProject_Machine\\Project_MachineLearning-1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "13:50:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:50:06 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet RMSE: 1543.4506400963041\n",
      "Training LSTM with target scaling...\n",
      "Final LSTM Loss (scaled): 0.014164\n",
      "LSTM trained and saved to models/lstm_baseline.keras\n"
     ]
    }
   ],
   "source": [
    "# Prophet example (fallback if not installed) and quick LSTM skeleton\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    prophet_available = True\n",
    "except Exception as e:\n",
    "    prophet_available = False\n",
    "    print('Prophet not available:', e)\n",
    "\n",
    "if prophet_available and 'df' in globals():\n",
    "    # robust creation of df for Prophet: ensure a ds and y\n",
    "    pdf = df.reset_index().rename(columns={df.index.name if df.index.name else df.columns[0]: 'ds'})\n",
    "    if close_col not in pdf.columns:\n",
    "        warnings.warn('Kolom target tidak ditemukan pada dataframe untuk Prophet.')\n",
    "    else:\n",
    "        pdf = pdf[['ds', close_col]].rename(columns={close_col: 'y'})\n",
    "        pdf = pdf.dropna()\n",
    "        train_size = int(len(pdf)*0.7)\n",
    "        p_train = pdf.iloc[:train_size]\n",
    "        p_test = pdf.iloc[train_size:]\n",
    "        try:\n",
    "            m = Prophet()\n",
    "            m.fit(p_train)\n",
    "            future = m.make_future_dataframe(periods=len(p_test), freq='D')\n",
    "            fcst = m.predict(future)\n",
    "            fc_pred = fcst.set_index('ds')['yhat'].iloc[-len(p_test):]\n",
    "            print('Prophet RMSE:', rmse(p_test['y'].values, fc_pred.values))\n",
    "        except Exception as e:\n",
    "            warnings.warn(f'Prophet run failed: {e}')\n",
    "\n",
    "# LSTM skeleton (tensorflow)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    tf_available = True\n",
    "except Exception as e:\n",
    "    tf_available = False\n",
    "    print('TensorFlow or Scaler not available:', e)\n",
    "\n",
    "if tf_available and 'df_feat' in globals() and len(df_feat) > 50:\n",
    "    # prepare simple windowed dataset\n",
    "    values = df_feat[close_col].values.reshape(-1, 1)\n",
    "    \n",
    "    # Scale the target to minimize loss magnitude\n",
    "    scaler_y = StandardScaler()\n",
    "    values_scaled = scaler_y.fit_transform(values).flatten()\n",
    "    \n",
    "    window = 21\n",
    "    Xw, yw = [], []\n",
    "    for i in range(window, len(values_scaled)):\n",
    "        Xw.append(values_scaled[i-window:i])\n",
    "        yw.append(values_scaled[i])\n",
    "    Xw = np.array(Xw)[..., np.newaxis]\n",
    "    yw = np.array(yw)\n",
    "    \n",
    "    # split\n",
    "    split = int(0.7*len(Xw))\n",
    "    Xtr, ytr = Xw[:split], yw[:split]\n",
    "    Xte, yte = Xw[split:], yw[split:]\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(window,1)),\n",
    "        tf.keras.layers.LSTM(32),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # EarlyStopping to find minimal loss automatically\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    print('Training LSTM with target scaling...')\n",
    "    history = model.fit(Xtr, ytr, epochs=50, validation_split=0.1, callbacks=[es], verbose=0)\n",
    "    print(f\"Final LSTM Loss (scaled): {history.history['loss'][-1]:.6f}\")\n",
    "    \n",
    "    # save model\n",
    "    model.save(MODELS_DIR / 'lstm_baseline.keras')\n",
    "    print('LSTM trained and saved to models/lstm_baseline.keras')\n",
    "else:\n",
    "    if tf_available:\n",
    "        warnings.warn('Data tidak cukup besar untuk LSTM (butuh >50 baris fitur).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d91d7b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: file://artifacts\\mlruns\n",
      "MLflow not available or failed: file://artifacts\\mlruns is not a valid remote uri. For remote access on windows, please consider using a different scheme such as SMB (e.g. smb://<hostname>/<path>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Documents\\MyProject_Machine\\Project_MachineLearning-1\\.venv\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    }
   ],
   "source": [
    "# MLflow quick setup & logging example (local)\n",
    "try:\n",
    "    import mlflow\n",
    "    mlflow.set_tracking_uri('file://' + str(ARTIFACTS_DIR / 'mlruns'))\n",
    "    print('MLflow tracking URI:', mlflow.get_tracking_uri())\n",
    "    with mlflow.start_run(run_name='rf_baseline'):\n",
    "        mlflow.log_param('model', 'RandomForest')\n",
    "        if 'rf_rmse' in globals() and rf_rmse is not None:\n",
    "            mlflow.log_metric('rf_rmse', float(rf_rmse))\n",
    "        else:\n",
    "            mlflow.log_metric('rf_rmse', -1)\n",
    "        if 'model_path' in globals() and Path(model_path).exists():\n",
    "            mlflow.log_artifact(str(model_path))\n",
    "        else:\n",
    "            warnings.warn('model_path tidak ada; tidak ada artefak model yang dilog.')\n",
    "except Exception as e:\n",
    "    print('MLflow not available or failed:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32157a9a",
   "metadata": {},
   "source": [
    "## Next steps & checklist\n",
    "\n",
    "- Validasi schema dengan `pandera`.\n",
    "- Tambahkan unit tests (`pytest`) untuk fungsi preprocessing & feature engineering.\n",
    "- Tambahkan CI (GitHub Actions) untuk lint, tests, build docker image.\n",
    "- Tambahkan Dockerfile dan FastAPI endpoint untuk `predict`.\n",
    "- Setup monitoring: drift detection dan retrain triggers.\n",
    "\n",
    "---\n",
    "\n",
    "**Catatan:** jalankan notebook cell per cell di environment yang benar; beberapa paket (Prophet, TensorFlow) mungkin membutuhkan dependencies OS tambahan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f807058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed data to artifacts\\preprocessed.parquet\n",
      "Saved feature list to artifacts\\feature_list.txt\n",
      "Prepared CNN dataset shapes (1935, 21, 13) (1935,)\n",
      "Saved scalers to artifacts/\n",
      "Training CNN with target scaling...\n",
      "Saved CNN model to models\\cnn_conv1d.keras\n"
     ]
    }
   ],
   "source": [
    "# CNN model skeleton (Conv1D) + save preprocessing artifacts\n",
    "# This cell does NOT run automatically; run manually after EDA & preprocessing cells.\n",
    "import joblib\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from keras import layers, models\n",
    "    tf_available = True\n",
    "except Exception as e:\n",
    "    tf_available = False\n",
    "    print('TensorFlow not available:', e)\n",
    "\n",
    "if 'df_feat' in globals():\n",
    "    # robustly determine target and features\n",
    "    if 'close_col' not in globals():\n",
    "        # try to infer\n",
    "        possible = [c for c in df_feat.columns if 'close' in c.lower() or 'adj' in c.lower()]\n",
    "        close_col_local = possible[0] if possible else df_feat.select_dtypes('number').columns[0]\n",
    "    else:\n",
    "        close_col_local = close_col\n",
    "    feature_cols = [c for c in df_feat.columns if c != close_col_local]\n",
    "\n",
    "    # Save preprocessed dataframe\n",
    "    ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    preproc_path = ARTIFACTS_DIR / 'preprocessed.parquet'\n",
    "    df_feat.to_parquet(preproc_path)\n",
    "    print('Saved preprocessed data to', preproc_path)\n",
    "\n",
    "    # Save feature list\n",
    "    (ARTIFACTS_DIR / 'feature_list.txt').write_text('\\n'.join(feature_cols))\n",
    "    print('Saved feature list to', ARTIFACTS_DIR / 'feature_list.txt')\n",
    "\n",
    "    # Prepare data for CNN: use sliding window on features to predict next close\n",
    "    window = 21\n",
    "    values_X = df_feat[feature_cols].values\n",
    "    values_y = df_feat[close_col_local].values.reshape(-1, 1) # Reshape for scaler\n",
    "    \n",
    "    # Fit scaler on target to minimize absolute loss value\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler_y = StandardScaler()\n",
    "    ys_scaled = scaler_y.fit_transform(values_y).flatten()\n",
    "\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window, len(ys_scaled)):\n",
    "        Xs.append(values_X[i-window:i])\n",
    "        ys.append(ys_scaled[i])\n",
    "    Xs = np.array(Xs)\n",
    "    ys = np.array(ys)\n",
    "    print('Prepared CNN dataset shapes', Xs.shape, ys.shape)\n",
    "\n",
    "    # Fit a StandardScaler on features and save\n",
    "    scaler_x = StandardScaler()\n",
    "    # reshape for scaler: combine windows\n",
    "    n_samples, n_steps, n_features = Xs.shape\n",
    "    Xs_2d = Xs.reshape(-1, n_features)\n",
    "    scaler_x.fit(Xs_2d)\n",
    "    joblib.dump(scaler_x, ARTIFACTS_DIR / 'scaler_x.joblib')\n",
    "    joblib.dump(scaler_y, ARTIFACTS_DIR / 'scaler_y.joblib')\n",
    "    print('Saved scalers to artifacts/')\n",
    "\n",
    "    # Optionally train a small Conv1D model if TF available\n",
    "    if tf_available and len(Xs) > 0:\n",
    "        # scale X\n",
    "        Xs_scaled = scaler_x.transform(Xs_2d).reshape(n_samples, n_steps, n_features)\n",
    "        split = int(0.7 * n_samples)\n",
    "        Xtr, Xte = Xs_scaled[:split], Xs_scaled[split:]\n",
    "        ytr, yte = ys[:split], ys[split:]\n",
    "\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=(n_steps, n_features)),\n",
    "            layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
    "            layers.MaxPooling1D(pool_size=2),\n",
    "            layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "            layers.GlobalAveragePooling1D(),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        # EarlyStopping for stability and minimal loss\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        print('Training CNN with target scaling...')\n",
    "        model.fit(Xtr, ytr, epochs=50, validation_data=(Xte, yte), callbacks=[es], verbose=0)\n",
    "        cnn_path = MODELS_DIR / 'cnn_conv1d.keras'\n",
    "        model.save(cnn_path)\n",
    "        print('Saved CNN model to', cnn_path)\n",
    "    else:\n",
    "        print('Skipping CNN training (TensorFlow not available).')\n",
    "else:\n",
    "    print('df_feat not found; run preprocessing/feature engineering cells first.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705e7d42",
   "metadata": {},
   "source": [
    "# MLOps Plan: KROM Bank Indonesia Stock - Notebook\n",
    "# Notebook ini berisi rencana MLOps, EDA cepat, baseline modeling, dan langkah produksi.\n",
    "# Dataset: files ditemukan di folder `dataset/` (mis: `BBSI.JK.csv`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3daa4e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> c:\\Users\\ASUS\\Documents\\MyProject_Machine\\Project_MachineLearning-1\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n",
      "> c:\\Users\\ASUS\\Documents\\MyProject_Machine\\Project_MachineLearning-1\\.venv\\Scripts\\python.exe -m pip install pandas numpy matplotlib seaborn scikit-learn statsmodels prophet>=1.1 tensorflow mlflow optuna joblib pandera lightgbm\n"
     ]
    }
   ],
   "source": [
    "## Persiapan lingkungan & install library\n",
    "\n",
    "# Perbarui pip dan install dependencies yang dibutuhkan. Jalankan cell ini di venv proyek (lihat `configure_python_environment`).\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def run(cmd):\n",
    "    print('>',' '.join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "# Update pip\n",
    "try:\n",
    "    run([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip'])\n",
    "except Exception as e:\n",
    "    print('Gagal update pip:', e)\n",
    "\n",
    "# Install packages (guarded installs)\n",
    "packages = [\n",
    "    'pandas', 'numpy', 'matplotlib', 'seaborn', 'scikit-learn', 'statsmodels',\n",
    "    'prophet>=1.1', 'tensorflow', 'mlflow', 'optuna', 'joblib', 'pandera', 'lightgbm'\n",
    "]\n",
    "try:\n",
    "    run([sys.executable, '-m', 'pip', 'install'] + packages)\n",
    "except Exception as e:\n",
    "    print('Instalasi paket gagal (lanjutkan jika sudah terpasang):', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82cc9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: C:\\Users\\ASUS\\Documents\\MyProject_Machine\\Project_MachineLearning-1\\dataset\n",
      "Found files: [WindowsPath('dataset/BBSI.JK.csv'), WindowsPath('dataset/BBSI.JK.parquet'), WindowsPath('dataset/BBSI.JK_monthly.csv'), WindowsPath('dataset/BBSI.JK_monthly.parquet'), WindowsPath('dataset/BBSI.JK_weekly.csv'), WindowsPath('dataset/BBSI.JK_weekly.parquet'), WindowsPath('dataset/README.txt'), WindowsPath('dataset/run-metadata.json'), WindowsPath('dataset/run-metadata_monthly.json'), WindowsPath('dataset/run-metadata_weekly.json')]\n"
     ]
    }
   ],
   "source": [
    "# Imports umum untuk notebook\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('.')\n",
    "DATA_DIR = ROOT / 'dataset'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "ARTIFACTS_DIR = ROOT / 'artifacts'\n",
    "for d in [DATA_DIR, MODELS_DIR, ARTIFACTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('DATA_DIR:', DATA_DIR.resolve())\n",
    "print('Found files:', list(DATA_DIR.glob('*'))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2074220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV candidates: ['dataset\\\\BBSI.JK.csv', 'dataset\\\\BBSI.JK_monthly.csv', 'dataset\\\\BBSI.JK_weekly.csv']\n",
      "Using CSV: dataset\\BBSI.JK.csv\n",
      "Loaded rows,cols: (1304, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ingested_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>549.837036</td>\n",
       "      <td>549.837036</td>\n",
       "      <td>458.197540</td>\n",
       "      <td>549.837036</td>\n",
       "      <td>549.837036</td>\n",
       "      <td>29516635</td>\n",
       "      <td>2026-02-04 06:58:01.058673+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-08</td>\n",
       "      <td>595.656799</td>\n",
       "      <td>687.296326</td>\n",
       "      <td>595.656799</td>\n",
       "      <td>687.296326</td>\n",
       "      <td>687.296326</td>\n",
       "      <td>19232861</td>\n",
       "      <td>2026-02-04 06:58:01.058673+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>733.116089</td>\n",
       "      <td>856.829407</td>\n",
       "      <td>687.296326</td>\n",
       "      <td>765.189880</td>\n",
       "      <td>765.189880</td>\n",
       "      <td>33271020</td>\n",
       "      <td>2026-02-04 06:58:01.058673+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>765.189880</td>\n",
       "      <td>797.263733</td>\n",
       "      <td>714.788147</td>\n",
       "      <td>714.788147</td>\n",
       "      <td>714.788147</td>\n",
       "      <td>1050747</td>\n",
       "      <td>2026-02-04 06:58:01.058673+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>668.968384</td>\n",
       "      <td>668.968384</td>\n",
       "      <td>668.968384</td>\n",
       "      <td>668.968384</td>\n",
       "      <td>668.968384</td>\n",
       "      <td>500439</td>\n",
       "      <td>2026-02-04 06:58:01.058673+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        open        high         low       close    adjclose  \\\n",
       "0  2020-09-07  549.837036  549.837036  458.197540  549.837036  549.837036   \n",
       "1  2020-09-08  595.656799  687.296326  595.656799  687.296326  687.296326   \n",
       "2  2020-09-09  733.116089  856.829407  687.296326  765.189880  765.189880   \n",
       "3  2020-09-10  765.189880  797.263733  714.788147  714.788147  714.788147   \n",
       "4  2020-09-11  668.968384  668.968384  668.968384  668.968384  668.968384   \n",
       "\n",
       "     volume                   ingested_at_utc  \n",
       "0  29516635  2026-02-04 06:58:01.058673+00:00  \n",
       "1  19232861  2026-02-04 06:58:01.058673+00:00  \n",
       "2  33271020  2026-02-04 06:58:01.058673+00:00  \n",
       "3   1050747  2026-02-04 06:58:01.058673+00:00  \n",
       "4    500439  2026-02-04 06:58:01.058673+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1304 entries, 0 to 1303\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Date             1304 non-null   object \n",
      " 1   open             1304 non-null   float64\n",
      " 2   high             1304 non-null   float64\n",
      " 3   low              1304 non-null   float64\n",
      " 4   close            1304 non-null   float64\n",
      " 5   adjclose         1304 non-null   float64\n",
      " 6   volume           1304 non-null   int64  \n",
      " 7   ingested_at_utc  1304 non-null   object \n",
      "dtypes: float64(5), int64(1), object(2)\n",
      "memory usage: 81.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper: cari CSV utama (prioritaskan daily CSV)\n",
    "from glob import glob\n",
    "csv_candidates = sorted(glob(str(DATA_DIR / '*.csv')))\n",
    "print('CSV candidates:', csv_candidates)\n",
    "\n",
    "# Preferensi: file yang mengandung '.JK.csv' atau the largest daily file\n",
    "def get_csv_path():\n",
    "    for p in csv_candidates:\n",
    "        if p.endswith('.csv') and '.JK' in os.path.basename(p) and '_monthly' not in p and '_weekly' not in p:\n",
    "            return Path(p)\n",
    "    # fallback: first csv\n",
    "    return Path(csv_candidates[0]) if csv_candidates else None\n",
    "\n",
    "CSV_PATH = get_csv_path()\n",
    "print('Using CSV:', CSV_PATH)\n",
    "\n",
    "# Muat CSV jika ada\n",
    "if CSV_PATH and CSV_PATH.exists():\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print('Loaded rows,cols:', df.shape)\n",
    "    display(df.head())\n",
    "    display(df.info())\n",
    "else:\n",
    "    print('CSV not found in dataset/, pastikan sudah diunduh.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca105935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw snapshot to dataset\\raw.csv\n",
      "\n",
      "Missing per column:\n",
      " Date               0\n",
      "open               0\n",
      "high               0\n",
      "low                0\n",
      "close              0\n",
      "adjclose           0\n",
      "volume             0\n",
      "ingested_at_utc    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1.304000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3690.913606</td>\n",
       "      <td>3763.538158</td>\n",
       "      <td>3612.549822</td>\n",
       "      <td>3701.924559</td>\n",
       "      <td>3701.924559</td>\n",
       "      <td>1.307254e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1172.449168</td>\n",
       "      <td>1187.231866</td>\n",
       "      <td>1149.150083</td>\n",
       "      <td>1170.789671</td>\n",
       "      <td>1170.789671</td>\n",
       "      <td>1.376887e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>508.599274</td>\n",
       "      <td>522.345215</td>\n",
       "      <td>458.197540</td>\n",
       "      <td>508.599274</td>\n",
       "      <td>508.599274</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3379.213074</td>\n",
       "      <td>3441.041504</td>\n",
       "      <td>3300.000000</td>\n",
       "      <td>3380.000000</td>\n",
       "      <td>3380.000000</td>\n",
       "      <td>3.400000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4020.000000</td>\n",
       "      <td>3900.752197</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>1.350000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4260.997803</td>\n",
       "      <td>4312.806519</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4280.000000</td>\n",
       "      <td>4280.000000</td>\n",
       "      <td>3.783675e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8070.102539</td>\n",
       "      <td>8070.102539</td>\n",
       "      <td>6817.845215</td>\n",
       "      <td>7328.024414</td>\n",
       "      <td>7328.024414</td>\n",
       "      <td>3.327102e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              open         high          low        close     adjclose  \\\n",
       "count  1304.000000  1304.000000  1304.000000  1304.000000  1304.000000   \n",
       "mean   3690.913606  3763.538158  3612.549822  3701.924559  3701.924559   \n",
       "std    1172.449168  1187.231866  1149.150083  1170.789671  1170.789671   \n",
       "min     508.599274   522.345215   458.197540   508.599274   508.599274   \n",
       "25%    3379.213074  3441.041504  3300.000000  3380.000000  3380.000000   \n",
       "50%    4000.000000  4020.000000  3900.752197  4000.000000  4000.000000   \n",
       "75%    4260.997803  4312.806519  4200.000000  4280.000000  4280.000000   \n",
       "max    8070.102539  8070.102539  6817.845215  7328.024414  7328.024414   \n",
       "\n",
       "             volume  \n",
       "count  1.304000e+03  \n",
       "mean   1.307254e+05  \n",
       "std    1.376887e+06  \n",
       "min    0.000000e+00  \n",
       "25%    3.400000e+03  \n",
       "50%    1.350000e+04  \n",
       "75%    3.783675e+04  \n",
       "max    3.327102e+07  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date columns candidates: ['Date']\n",
      "Date range: 2020-09-07 00:00:00 to 2026-02-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Simpan salinan raw\n",
    "RAW_PATH = DATA_DIR / 'raw.csv'\n",
    "if 'df' in globals():\n",
    "    df.to_csv(RAW_PATH, index=False)\n",
    "    print('Saved raw snapshot to', RAW_PATH)\n",
    "\n",
    "# Basic EDA: missing, describe, date range\n",
    "if 'df' in globals():\n",
    "    print('\\nMissing per column:\\n', df.isna().sum())\n",
    "    display(df.describe())\n",
    "    # Cari kolom tanggal yang paling umum (case-insensitive)\n",
    "    date_cols = [c for c in df.columns if 'date' in c.lower() or 'tanggal' in c.lower()]\n",
    "    print('Date columns candidates:', date_cols)\n",
    "    if date_cols:\n",
    "        date_col = date_cols[0]\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        if df[date_col].isna().all():\n",
    "            warnings.warn(f\"Kolom '{date_col}' tidak dapat diparse ke datetime (semua NaT). Periksa kolom lain atau format tanggal.\")\n",
    "        else:\n",
    "            print('Date range:', df[date_col].min(), 'to', df[date_col].max())\n",
    "    else:\n",
    "        warnings.warn('Tidak menemukan kolom tanggal otomatis; pastikan file memiliki kolom tanggal.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38993cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_daily shape: (1976, 6)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: set date index, sort, resample daily, fill missing\n",
    "# This cell creates `df_daily` used by feature engineering.\n",
    "if 'df' in globals():\n",
    "    # detect date column again (from EDA)\n",
    "    possible_dates = [c for c in df.columns if 'date' in c.lower() or 'tanggal' in c.lower()]\n",
    "    date_col = possible_dates[0] if possible_dates else df.columns[0]\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    if df[date_col].isna().sum() > 0:\n",
    "        warnings.warn('Beberapa tanggal tidak ter-parse; baris akan di-drop sebelum resample.')\n",
    "        df = df.loc[df[date_col].notna()].copy()\n",
    "    df = df.sort_values(date_col).set_index(date_col)\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    if not numeric_cols:\n",
    "        warnings.warn('Tidak ada kolom numerik; feature engineering akan dibatalkan.')\n",
    "    else:\n",
    "        # resample ke harian dan forward-fill untuk missing timestamps\n",
    "        df_daily = df[numeric_cols].resample('D').ffill()\n",
    "        print('df_daily shape:', df_daily.shape)\n",
    "else:\n",
    "    warnings.warn('`df` tidak ditemukan; pastikan CSV sudah dimuat.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a671d977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>return</th>\n",
       "      <th>sma_7</th>\n",
       "      <th>sma_21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-09-27</th>\n",
       "      <td>568.164978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>549.182491</td>\n",
       "      <td>599.802399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-28</th>\n",
       "      <td>586.492859</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>560.310146</td>\n",
       "      <td>601.547914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>604.820740</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>572.092364</td>\n",
       "      <td>597.620505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>636.894592</td>\n",
       "      <td>0.053030</td>\n",
       "      <td>584.529157</td>\n",
       "      <td>591.511206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01</th>\n",
       "      <td>623.148682</td>\n",
       "      <td>-0.021583</td>\n",
       "      <td>593.693115</td>\n",
       "      <td>587.147422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 close    return       sma_7      sma_21\n",
       "Date                                                    \n",
       "2020-09-27  568.164978  0.000000  549.182491  599.802399\n",
       "2020-09-28  586.492859  0.032258  560.310146  601.547914\n",
       "2020-09-29  604.820740  0.031250  572.092364  597.620505\n",
       "2020-09-30  636.894592  0.053030  584.529157  591.511206\n",
       "2020-10-01  623.148682 -0.021583  593.693115  587.147422"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature engineering: returns, lags, rolling\n",
    "if 'df_daily' in globals():\n",
    "    df_feat = df_daily.copy()\n",
    "    # ensure 'Close' exists\n",
    "    close_col = None\n",
    "    for c in ['Close','close','Adj Close','adj_close','Close*']:\n",
    "        if c in df_feat.columns:\n",
    "            close_col = c\n",
    "            break\n",
    "    if close_col is None:\n",
    "        # fallback to first numeric column\n",
    "        close_col = df_feat.select_dtypes('number').columns[0]\n",
    "        warnings.warn(f\"Tidak menemukan kolom 'Close' eksplisit. Menggunakan '{close_col}' sebagai target.\")\n",
    "    # require enough data for lags and rolling calculations\n",
    "    min_required = 21\n",
    "    if len(df_feat) < min_required:\n",
    "        raise ValueError(f\"Data terlalu sedikit untuk feature engineering (butuh >= {min_required}, punya {len(df_feat)})\")\n",
    "    df_feat['return'] = df_feat[close_col].pct_change()\n",
    "    for lag in [1,2,3,5,7]:\n",
    "        df_feat[f'lag_{lag}'] = df_feat[close_col].shift(lag)\n",
    "    df_feat['sma_7'] = df_feat[close_col].rolling(7).mean()\n",
    "    df_feat['sma_21'] = df_feat[close_col].rolling(21).mean()\n",
    "    df_feat = df_feat.dropna().copy()\n",
    "    display(df_feat[[close_col,'return','sma_7','sma_21']].head())\n",
    "else:\n",
    "    warnings.warn('`df_daily` tidak tersedia; jalankan sel preprocessing terlebih dahulu.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e982847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test sizes: 1369 293 294\n",
      "Naive metrics: {'rmse': 97.58826460059171, 'mae': 36.42857142857143, 'mape': 0.8639266977681075}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_26620\\3697970242.py:37: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  naive_pred = y_test.shift(1).fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "# Train/val/test split (time-series aware) and baseline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# safe metric functions\n",
    "def rmse(a,b):\n",
    "    return sqrt(mean_squared_error(a,b))\n",
    "\n",
    "def mape(a,b):\n",
    "    a = np.array(a, dtype=float)\n",
    "    b = np.array(b, dtype=float)\n",
    "    # avoid division by zero: ignore positions where a == 0\n",
    "    mask = a != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((a[mask]-b[mask])/a[mask]))*100\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'rmse': rmse(y_true, y_pred),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'mape': mape(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "if 'df_feat' in globals():\n",
    "    y = df_feat[close_col]\n",
    "    X = df_feat.drop(columns=[close_col])\n",
    "    n = len(df_feat)\n",
    "    train_end = int(n*0.7)\n",
    "    val_end = train_end + int(n*0.15)\n",
    "    X_train, y_train = X.iloc[:train_end], y.iloc[:train_end]\n",
    "    X_val, y_val = X.iloc[train_end:val_end], y.iloc[train_end:val_end]\n",
    "    X_test, y_test = X.iloc[val_end:], y.iloc[val_end:]\n",
    "    print('Train/Val/Test sizes:', len(X_train), len(X_val), len(X_test))\n",
    "\n",
    "# Baseline: naive forecast (predict t+1 = t)\n",
    "if 'y_test' in globals() and len(y_test)>0:\n",
    "    naive_pred = y_test.shift(1).fillna(method='bfill')\n",
    "    print('Naive metrics:', compute_metrics(y_test.values, naive_pred.values))\n",
    "else:\n",
    "    print('Tidak ada data test untuk mengevaluasi baseline.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ff948c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF metrics: {'rmse': 11.228325775936217, 'mae': 8.874453623246172, 'mape': 0.21877368484402945}\n",
      "Saved model to models\\rf_baseline.pkl\n"
     ]
    }
   ],
   "source": [
    "# RandomForest baseline + save model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "rf_rmse = None\n",
    "\n",
    "if 'X_train' in globals() and len(X_test)>0:\n",
    "    pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    metrics = compute_metrics(y_test.values, preds)\n",
    "    rf_rmse = metrics['rmse']\n",
    "    print('RF metrics:', metrics)\n",
    "    model_path = MODELS_DIR / f'rf_baseline.pkl'\n",
    "    joblib.dump(pipe, model_path)\n",
    "    print('Saved model to', model_path)\n",
    "else:\n",
    "    warnings.warn('Tidak cukup data untuk melatih RandomForest baseline.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b5d7f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:15:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:15:36 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet RMSE: 1543.4506400963041\n",
      "Epoch 1/5\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 14197701.0000 - val_loss: 19997908.0000\n",
      "Epoch 2/5\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 14193234.0000 - val_loss: 19994142.0000\n",
      "Epoch 3/5\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 14191098.0000 - val_loss: 19991616.0000\n",
      "Epoch 4/5\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 14189096.0000 - val_loss: 19989088.0000\n",
      "Epoch 5/5\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 14187100.0000 - val_loss: 19986576.0000\n",
      "LSTM trained and saved\n"
     ]
    }
   ],
   "source": [
    "# Prophet example (fallback if not installed) and quick LSTM skeleton\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    prophet_available = True\n",
    "except Exception as e:\n",
    "    prophet_available = False\n",
    "    print('Prophet not available:', e)\n",
    "\n",
    "if prophet_available and 'df' in globals():\n",
    "    # robust creation of df for Prophet: ensure a ds and y\n",
    "    pdf = df.reset_index().rename(columns={df.index.name if df.index.name else df.columns[0]: 'ds'})\n",
    "    if close_col not in pdf.columns:\n",
    "        warnings.warn('Kolom target tidak ditemukan pada dataframe untuk Prophet.')\n",
    "    else:\n",
    "        pdf = pdf[['ds', close_col]].rename(columns={close_col: 'y'})\n",
    "        pdf = pdf.dropna()\n",
    "        train_size = int(len(pdf)*0.7)\n",
    "        p_train = pdf.iloc[:train_size]\n",
    "        p_test = pdf.iloc[train_size:]\n",
    "        try:\n",
    "            m = Prophet()\n",
    "            m.fit(p_train)\n",
    "            future = m.make_future_dataframe(periods=len(p_test), freq='D')\n",
    "            fcst = m.predict(future)\n",
    "            fc_pred = fcst.set_index('ds')['yhat'].iloc[-len(p_test):]\n",
    "            print('Prophet RMSE:', rmse(p_test['y'].values, fc_pred.values))\n",
    "        except Exception as e:\n",
    "            warnings.warn(f'Prophet run failed: {e}')\n",
    "\n",
    "# LSTM skeleton (tensorflow)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    tf_available = True\n",
    "except Exception as e:\n",
    "    tf_available = False\n",
    "    print('TensorFlow not available:', e)\n",
    "\n",
    "if tf_available and 'df_feat' in globals() and len(df_feat) > 50:\n",
    "    # prepare simple windowed dataset\n",
    "    values = df_feat[close_col].values\n",
    "    window = 21\n",
    "    Xw, yw = [], []\n",
    "    for i in range(window, len(values)):\n",
    "        Xw.append(values[i-window:i])\n",
    "        yw.append(values[i])\n",
    "    Xw = np.array(Xw)[..., np.newaxis]\n",
    "    yw = np.array(yw)\n",
    "    # split\n",
    "    split = int(0.7*len(Xw))\n",
    "    Xtr, ytr = Xw[:split], yw[:split]\n",
    "    Xte, yte = Xw[split:], yw[split:]\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(window,1)),\n",
    "        tf.keras.layers.LSTM(32),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    # NOTE: training LSTM can be slow; ensure you have resources\n",
    "    # train briefly\n",
    "    history = model.fit(Xtr, ytr, epochs=5, validation_split=0.1)\n",
    "    # save model (add .keras extension as required by Keras 3.x)\n",
    "    model.save(MODELS_DIR / 'lstm_baseline.keras')\n",
    "    print('LSTM trained and saved')\n",
    "else:\n",
    "    if tf_available:\n",
    "        warnings.warn('Data tidak cukup besar untuk LSTM (butuh >50 baris fitur).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d91d7b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: file://artifacts\\mlruns\n",
      "MLflow not available or failed: file://artifacts\\mlruns is not a valid remote uri. For remote access on windows, please consider using a different scheme such as SMB (e.g. smb://<hostname>/<path>).\n"
     ]
    }
   ],
   "source": [
    "# MLflow quick setup & logging example (local)\n",
    "try:\n",
    "    import mlflow\n",
    "    mlflow.set_tracking_uri('file://' + str(ARTIFACTS_DIR / 'mlruns'))\n",
    "    print('MLflow tracking URI:', mlflow.get_tracking_uri())\n",
    "    with mlflow.start_run(run_name='rf_baseline'):\n",
    "        mlflow.log_param('model', 'RandomForest')\n",
    "        if 'rf_rmse' in globals() and rf_rmse is not None:\n",
    "            mlflow.log_metric('rf_rmse', float(rf_rmse))\n",
    "        else:\n",
    "            mlflow.log_metric('rf_rmse', -1)\n",
    "        if 'model_path' in globals() and Path(model_path).exists():\n",
    "            mlflow.log_artifact(str(model_path))\n",
    "        else:\n",
    "            warnings.warn('model_path tidak ada; tidak ada artefak model yang dilog.')\n",
    "except Exception as e:\n",
    "    print('MLflow not available or failed:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32157a9a",
   "metadata": {},
   "source": [
    "## Next steps & checklist\n",
    "\n",
    "- Validasi schema dengan `pandera`.\n",
    "- Tambahkan unit tests (`pytest`) untuk fungsi preprocessing & feature engineering.\n",
    "- Tambahkan CI (GitHub Actions) untuk lint, tests, build docker image.\n",
    "- Tambahkan Dockerfile dan FastAPI endpoint untuk `predict`.\n",
    "- Setup monitoring: drift detection dan retrain triggers.\n",
    "\n",
    "---\n",
    "\n",
    "**Catatan:** jalankan notebook cell per cell di environment yang benar; beberapa paket (Prophet, TensorFlow) mungkin membutuhkan dependencies OS tambahan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f807058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed data to artifacts\\preprocessed.parquet\n",
      "Saved feature list to artifacts\\feature_list.txt\n",
      "Prepared CNN dataset shapes (1935, 21, 13) (1935,)\n",
      "Saved scaler to artifacts\\scaler.joblib\n",
      "Epoch 1/5\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 14683543.0000 - val_loss: 16616559.0000\n",
      "Epoch 2/5\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 13024005.0000 - val_loss: 14014591.0000\n",
      "Epoch 3/5\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7436776.5000 - val_loss: 6900344.5000\n",
      "Epoch 4/5\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 4903364.5000 - val_loss: 5930897.0000\n",
      "Epoch 5/5\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4542682.0000 - val_loss: 5439797.5000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=models\\cnn_conv1d.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     73\u001b[39m     model.fit(Xtr, ytr, epochs=\u001b[32m5\u001b[39m, validation_data=(Xte, yte))\n\u001b[32m     74\u001b[39m     cnn_path = MODELS_DIR / \u001b[33m'\u001b[39m\u001b[33mcnn_conv1d\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSaved CNN model to\u001b[39m\u001b[33m'\u001b[39m, cnn_path)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Documents\\MyProject_Machine\\Project_MachineLearning-1\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Documents\\MyProject_Machine\\Project_MachineLearning-1\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:114\u001b[39m, in \u001b[36msave_model\u001b[39m\u001b[34m(model, filepath, overwrite, zipped, **kwargs)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith((\u001b[33m\"\u001b[39m\u001b[33m.h5\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.hdf5\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format.save_model_to_hdf5(\n\u001b[32m    112\u001b[39m         model, filepath, overwrite, include_optimizer\n\u001b[32m    113\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    115\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mInvalid filepath extension for saving. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPlease add either a `.keras` extension for the native Keras \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mformat (recommended) or a `.h5` extension. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUse `model.export(filepath)` if you want to export a SavedModel \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfor use with TFLite/TFServing/etc. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    121\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=models\\cnn_conv1d."
     ]
    }
   ],
   "source": [
    "# CNN model skeleton (Conv1D) + save preprocessing artifacts\n",
    "# This cell does NOT run automatically; run manually after EDA & preprocessing cells.\n",
    "import joblib\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from keras import layers, models\n",
    "    tf_available = True\n",
    "except Exception as e:\n",
    "    tf_available = False\n",
    "    print('TensorFlow not available:', e)\n",
    "\n",
    "if 'df_feat' in globals():\n",
    "    # robustly determine target and features\n",
    "    if 'close_col' not in globals():\n",
    "        # try to infer\n",
    "        possible = [c for c in df_feat.columns if 'close' in c.lower() or 'adj' in c.lower()]\n",
    "        close_col_local = possible[0] if possible else df_feat.select_dtypes('number').columns[0]\n",
    "    else:\n",
    "        close_col_local = close_col\n",
    "    feature_cols = [c for c in df_feat.columns if c != close_col_local]\n",
    "\n",
    "    # Save preprocessed dataframe\n",
    "    ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    preproc_path = ARTIFACTS_DIR / 'preprocessed.parquet'\n",
    "    df_feat.to_parquet(preproc_path)\n",
    "    print('Saved preprocessed data to', preproc_path)\n",
    "\n",
    "    # Save feature list\n",
    "    (ARTIFACTS_DIR / 'feature_list.txt').write_text('\\n'.join(feature_cols))\n",
    "    print('Saved feature list to', ARTIFACTS_DIR / 'feature_list.txt')\n",
    "\n",
    "    # Prepare data for CNN: use sliding window on features to predict next close\n",
    "    window = 21\n",
    "    values_X = df_feat[feature_cols].values\n",
    "    values_y = df_feat[close_col_local].values\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window, len(values_y)):\n",
    "        Xs.append(values_X[i-window:i])\n",
    "        ys.append(values_y[i])\n",
    "    Xs = np.array(Xs)\n",
    "    ys = np.array(ys)\n",
    "    print('Prepared CNN dataset shapes', Xs.shape, ys.shape)\n",
    "\n",
    "    # Fit a StandardScaler on features and save\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # reshape for scaler: combine windows\n",
    "    n_samples, n_steps, n_features = Xs.shape\n",
    "    Xs_2d = Xs.reshape(-1, n_features)\n",
    "    scaler.fit(Xs_2d)\n",
    "    joblib.dump(scaler, ARTIFACTS_DIR / 'scaler.joblib')\n",
    "    print('Saved scaler to', ARTIFACTS_DIR / 'scaler.joblib')\n",
    "\n",
    "    # Optionally train a small Conv1D model if TF available\n",
    "    if tf_available and len(Xs) > 0:\n",
    "        # scale X\n",
    "        Xs_scaled = scaler.transform(Xs_2d).reshape(n_samples, n_steps, n_features)\n",
    "        split = int(0.7 * n_samples)\n",
    "        Xtr, Xte = Xs_scaled[:split], Xs_scaled[split:]\n",
    "        ytr, yte = ys[:split], ys[split:]\n",
    "\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=(n_steps, n_features)),\n",
    "            layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
    "            layers.MaxPooling1D(pool_size=2),\n",
    "            layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "            layers.GlobalAveragePooling1D(),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        # NOTE: keep epochs small for smoke test\n",
    "        model.fit(Xtr, ytr, epochs=5, validation_data=(Xte, yte))\n",
    "        cnn_path = MODELS_DIR / 'cnn_conv1d.keras'\n",
    "        model.save(cnn_path)\n",
    "        print('Saved CNN model to', cnn_path)\n",
    "    else:\n",
    "        print('Skipping CNN training (TensorFlow not available).')\n",
    "else:\n",
    "    print('df_feat not found; run preprocessing/feature engineering cells first.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
